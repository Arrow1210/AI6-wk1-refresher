{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Data Manipulation and Analysis with Pandas\n",
    "\n",
    "![](http://pandas.pydata.org/_static/pandas_logo.png)\n",
    "[Pandas](http://pandas.pydata.org/) is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. Pandas is free software released under the three-clause BSD license. The name is derived from the term _panel data_, an econometrics term for multidimensional structured data sets.\n",
    "\n",
    "#### Contents\n",
    "\n",
    "* [Series](#Series)\n",
    "* [DataFrames](#DataFrames)\n",
    "* [Importing Pandas](#Importing-Pandas) and other libraries.\n",
    "* [Creating Data](#Creating-Data) using lists and tuples\n",
    "* [Viewing Data](#Viewing-Data)\n",
    "* [Saving Data](#Saving-Data) to_csv and to_excel\n",
    "* [Loading Data](#Loading-Data) read_csv, read_table read_excel, read_html\n",
    "    * [Unix and os](#Unix-and-os)\n",
    "    * [CSVs and Excel](#CSVs-and-Excel)\n",
    "* [Selecting Data](#Selecting-Data) loc,iloc,isin\n",
    "    * [Masks](#Masks) or boolean arrays\n",
    "* [Preparing Data](#Preparing-Data)\n",
    "    * [Missing Values](#Missing-Values)\n",
    "    \n",
    "*[to be continued in part 4b]*\n",
    "\n",
    "NB: This notebook misses some methods of joining and concatenating and merging data. The instances in which those are useful are quite specific, so we'll see some examples but won't have a section in this notebook for reference. \n",
    "\n",
    "#### Resources:  \n",
    "* [Pandas Documentation](http://pandas.pydata.org/pandas-docs/stable/index.html), especially\n",
    "[10 minutes to pandas](http://pandas.pydata.org/pandas-docs/stable/10min.html)  \n",
    "* [The Data Incubator](https://www.thedataincubator.com/)  \n",
    "* [Hernan Rojas' learn-pandas](https://bitbucket.org/hrojas/learn-pandas)  \n",
    "* [Harvard CS109 lab1 content](https://github.com/cs109/2015lab1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas library as pd\n",
    "import pandas as pd\n",
    "#import matplotlib library as plt \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pandas series from list\n",
    "#observe the dtype\n",
    "animals = ['Tiger', 'Bear', 'Moose']\n",
    "pd.Series(animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pandas series from list\n",
    "#observe the dtype\n",
    "numbers = [1, 2, 3]\n",
    "pd.Series(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is possible to create a pandas series with None, indicating no data\n",
    "#observe the dtype\n",
    "animals = ['Tiger', 'Bear', None]\n",
    "pd.Series(animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the case of numbers, None will give a NaN (Not a Number)\n",
    "#observe the dtype\n",
    "numbers = [1, 2, None]\n",
    "pd.Series(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pandas series from dictionary\n",
    "sports = {'Archery': 'Bhutan',\n",
    "          'Golf': 'Scotland',\n",
    "          'Sumo': 'Japan',\n",
    "          'Taekwondo': 'South Korea'}\n",
    "s = pd.Series(sports)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what's the type for s?\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice the index for s\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also create a series from list and index\n",
    "sr = pd.Series(['Tiger', 'Bear', 'Moose'], index=['India', 'America', 'Canada'])\n",
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a list can be created based on the specified indices\n",
    "sports = {'Archery': 'Bhutan',\n",
    "          'Golf': 'Scotland',\n",
    "          'Sumo': 'Japan',\n",
    "          'Taekwondo': 'South Korea'}\n",
    "sp1 = pd.Series(sports, index=['Golf', 'Sumo', 'Hockey'])\n",
    "sp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = {'Archery': 'Bhutan',\n",
    "          'Golf': 'Scotland',\n",
    "          'Sumo': 'Japan',\n",
    "          'Taekwondo': 'South Korea'}\n",
    "s = pd.Series(sports)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iloc to query based on index\n",
    "s.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc to query based on location\n",
    "s.loc['Golf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to query based on index\n",
    "s[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to query based on location\n",
    "s['Golf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports = {99: 'Bhutan',\n",
    "          100: 'Scotland',\n",
    "          101: 'Japan',\n",
    "          102: 'South Korea'}\n",
    "s = pd.Series(sports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[0] #This won't call s.iloc[0] as one might expect, it generates an error instead\n",
    "\n",
    "# try s[99] or s[101]. What can you make of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pandas series from a dictionary and a list -- flexible!\n",
    "original_sports = pd.Series({'Archery': 'Bhutan',\n",
    "                             'Golf': 'Scotland',\n",
    "                             'Sumo': 'Japan',\n",
    "                             'Taekwondo': 'South Korea'})\n",
    "\n",
    "cricket_loving_countries = pd.Series(['Australia',\n",
    "                                      'Barbados',\n",
    "                                      'Pakistan',\n",
    "                                      'England'], \n",
    "                                index=['Cricket',\n",
    "                                       'Cricket',\n",
    "                                       'Cricket',\n",
    "                                       'Cricket'])\n",
    "all_countries = original_sports.append(cricket_loving_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using loc to list all countries with the index 'Cricket'\n",
    "all_countries.loc['Cricket']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "\n",
    "A data frame is like a table, with rows and columns (e.g., as in SQL or Excel).  \n",
    "**Except** that :\n",
    "  - The rows can be indexed by something interesting (there is special support for labels like categorical and timeseries data).\n",
    "  - Cells can store any Python object. Like in SQL, columns must have a homogenous type.\n",
    "  - Instead of \"NULL\", the name for a non-existent value is \"NA\".  Unlike R, Python's data frames only support NAs in columns of some data types (basically: floating point numbers and 'objects') -- but this is mostly a non-issue (because it will \"up-type\" integers to float64, etc.)\n",
    "  \n",
    "Each of a ```DataFrame```'s columns are an individual ```Series```, (more correctly, a dataframe is a dictionary of Series).  The entires series must have a homogenous type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "# Let's make a dataset that consists of Malaysian States\n",
    "# and the size of each state in km2.  Let's try and rank \n",
    "# the states of Malaysia by land area, and figure out if East\n",
    "# Malaysia is larger or smaller than West Malaysia\n",
    "\n",
    "states = ['Johor','Kedah','Kelantan','Melaka', \n",
    "          'Negeri Sembilan','Pahang','Perak','Perlis',\n",
    "          'Penang','Sabah', 'Sarawak','Selangor','Terengganu']\n",
    "area = [19210,9500,15099,1664,6686,36137,21035,\n",
    "        821,1048,73631,124450,8104,13035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the zip function to merge the two lists together\n",
    "zip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Data Set\n",
    "state_area = list(zip(states, area))\n",
    "state_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will use the ***pandas*** library to export this data set into a csv file. \n",
    "\n",
    "***df*** will be a ***DataFrame*** object. You can think of this object holding the contents of states in a format similar to a sql table or an excel spreadsheet. Let's take a look below at the contents inside ***df***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = state_area, columns=['State', 'Area'])\n",
    "#head to show first 5 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((type(df)))\n",
    "print() \n",
    "print((df.info()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide statistical summary for the whole dataset\n",
    "df.describe()\n",
    "#find out what these statistics are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking of land area -- top 5\n",
    "# East Malaysia looks big!\n",
    "df = df.sort_values('Area', ascending=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding WPs, East Malaysia is larger than West Malaysia!\n",
    "print(('East Malaysia Size: ', df['Area'][9:11].sum()))           # get only data from row 9 and 10 to be summed\n",
    "print(('West Malaysia Size: ', df['Area'].sum() - df['Area'][9:11].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph\n",
    "df['Area'].plot.bar()\n",
    "plt.xticks(np.arange(13), (df['State']))\n",
    "plt.xlabel('Malaysian States')\n",
    "plt.ylabel('Land Area in $km^2$')\n",
    "plt.title('Malaysian States by Land Area')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we proceed to the second example, let's store the DataFrame 'df' in another variable\n",
    "stateDF = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "purchase_1 = pd.Series({'Name': 'Chris',\n",
    "                        'Item Purchased': 'Dog Food',\n",
    "                        'Cost': 22.50})\n",
    "purchase_2 = pd.Series({'Name': 'Kevyn',\n",
    "                        'Item Purchased': 'Kitty Litter',\n",
    "                        'Cost': 2.50})\n",
    "purchase_3 = pd.Series({'Name': 'Vinod',\n",
    "                        'Item Purchased': 'Bird Seed',\n",
    "                        'Cost': 5.00})\n",
    "df = pd.DataFrame([purchase_1, purchase_2, purchase_3], index=['Store 1', 'Store 1', 'Store 2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Store 2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.loc['Store 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Store 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Store 1', 'Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose dataframe\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.loc['Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['Store 1']['Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,['Name', 'Cost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to delete a row\n",
    "df.drop('Store 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that drop does not change the original dataset\n",
    "#drop created a copy instead\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df = df.copy()\n",
    "copy_df = copy_df.drop('Store 1')\n",
    "copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use del\n",
    "#will change the original dataset\n",
    "del copy_df['Name']\n",
    "copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column 'Location' with None value\n",
    "df['Location'] = None\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Pandas\n",
    "\n",
    "The general way to import libraries is to write\n",
    "```python\n",
    "import library #import the library directly\n",
    "import library as alias \n",
    "\n",
    "# This just aliases the package names.\n",
    "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
    "\n",
    "from library import function # import specific functions or types in a library\n",
    "%jupyter magic # jupyter only functions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports - for style reasons, try and put in alphabetical order, unless there are subgroupings of imports\n",
    "# that you want.\n",
    "import matplotlib #we'll only use this to determine the matplotlib version number\n",
    "import matplotlib.pyplot as plt  # the graphing library\n",
    "import numpy as np # scientific computing library\n",
    "import pandas as pd # the data structure and analysis library\n",
    "from pandas import DataFrame, read_csv, Series # specific functions from pandas\n",
    "import seaborn as sns # Makes graphs look pretty\n",
    "import sys #we'll only use this to determine the python version number\n",
    "\n",
    "# Enable inline plotting.  The % is an iPython thing, and is not part of the Python language.\n",
    "# In this case we're just telling the plotting library to draw things on\n",
    "# the notebook, instead of on a separate window.\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the imports are listed as modules, including pyplot.  But there are several other types\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to check your version numbers\n",
    "print(('Python version: ' + sys.version))\n",
    "print() \n",
    "print(('Pandas version: ' + pd.__version__))\n",
    "print(('Matplotlib version: ' + matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data\n",
    "\n",
    "There are many ways to input data into Pandas. The goal of this is to input data to DataFrames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A data frame, using a dictionary with ordered \n",
    "# lists for columns\n",
    "\n",
    "df1 = pd.DataFrame({\n",
    "    'number': [1, 2, 3],\n",
    "    'animal': ['cat', 'dog', 'mouse']\n",
    "})\n",
    "\n",
    "# The same data frame, using tuples for each row\n",
    "# We need to give the column names separately!\n",
    "df2 = pd.DataFrame([\n",
    "    ('cat', 1),\n",
    "    ('dog', 2),\n",
    "    ('mouse', 3),\n",
    "], columns=['animal', 'number'])\n",
    "\n",
    "# Are they the same?\n",
    "assert((df1 == df2).all().all)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20161101',periods =6)\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Create another DataFrame, with different data types.  \n",
    "Side note that that you can **copy** examples from the internet \n",
    "like this into Jupyter Notebook, it will still works, but only if there isn't anything else in the cell!\n",
    "\n",
    "From: http://pandas.pydata.org/pandas-docs/stable/10min.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [10]: df2 = pd.DataFrame({ 'A' : 1.,\n",
    "   ....:                      'B' : pd.Timestamp('20130102'),\n",
    "   ....:                      'C' : pd.Series(1,index=list(range(4)),dtype='float32'),\n",
    "   ....:                      'D' : np.array([3] * 4,dtype='int32'),\n",
    "   ....:                      'E' : pd.Categorical([\"test\",\"train\",\"test\",\"train\"]),\n",
    "   ....:                      'F' : 'foo' })\n",
    "   ....: \n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! It works huh? (Even with all the extra stuff at the margin side...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Columns have specific data types\n",
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a single column, which yields a Series.\n",
    "\n",
    "A Series, like a numpy array, most be of homogenous type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like Dictionaries! DataFrame is a dictionary of Series!\n",
    "df1['animal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also access a column as a property of df1\n",
    "df1.animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can access rows like how you would with lists...\n",
    "df3[3:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "#### Exporting to CSVs and Excel files\n",
    "\n",
    "Let's revive back our earlier example on the Malaysian states and export the DataFrame `df` to a ***csv*** file. We can name the file ***malaysia_states.csv***, but we can also do a txt file! The function ***to_csv*** will be used to export the file. The file will be saved in the same location of the notebook unless specified otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy back from stateDF (which has the Malaysian states data. There's actually no purpose in doing this but \n",
    "# just to keep the variable short :) \n",
    "df = stateDF   \n",
    "df.to_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only parameters we will use is ***index*** and ***header***. Setting these parameters to True will prevent the index and header names from being exported. Change the values of these parameters to get a better understanding of their use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/malaysia_states.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also try a text file\n",
    "# CSV actually stands for comma separated values, which can be opened as a text file \n",
    "df.to_csv('data/malaysia_states.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And to Excel files\n",
    "df.to_excel('data/malaysia_states.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset our namespace; delete all variables\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import again\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy.random as random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Let's now try accessing that csv that we just saved.  Let us take a look at this function and what inputs it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this functions has many parameters, we will simply pass it the location of the text file. We know that we saved things into the same directory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSVs and Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'data/malaysia_states.csv'    # why is there an 'r' in front of the filename?\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv` function treated the first record in the csv file as the header names. This is obviously not correct since the text file did not provide us with header names.\n",
    "To correct this we will pass the header parameter to the `read_csv` function and set it to None (means null in python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to give the columns specific names, we would have to pass another parameter called `names`. We can also omit the header parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_df = pd.read_csv(path, names=['State','Area'])\n",
    "area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the numbers [0,1,2,3,4] as the row numbers in an Excel file. In pandas these are part of the ***index*** of the dataframe. You can think of the index as the primary key of a SQL table with the exception that an index is allowed to have duplicates.  \n",
    "\n",
    "***[State, Area]*** can be thought of as column headers similar to the ones typically found in an Excel spreadsheet or SQL database.\n",
    "\n",
    "Now, to delete the csv file now that we are done using it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a Python Library - you can also use the unix command directly! Easy!\n",
    "import os\n",
    "os.remove(path)   # we had the filename stored in 'path' earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we do the same with xls files, only use read_excel.\n",
    "# Try it!\n",
    "pd.read_excel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data\n",
    "\n",
    "We can select data both by their labels and by their position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('20160101', periods=6)\n",
    "df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))\n",
    "print(dates)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try head, tail, index, columns, values, describe, T, sort_index\n",
    "# sort_values and see for yourself what they do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a cross section on a label (or in other words, a row)\n",
    "print((dates[0]))\n",
    "print((df.loc[dates[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting on a multi-axis by label\n",
    "df.loc[:,['A','B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing label slicing, both endpoints are included unlike normal slicing\n",
    "df.loc['20160103':'20160105',['B','C']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a scalar value... both work!\n",
    "df.loc[dates[2],'D']\n",
    "#df.at[dates[2],'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```iloc``` is the same as ```loc```, only it works by position, not by label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select via the position of the passed integers\n",
    "df.iloc[2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By integer slices, acting similarly to numpy/python\n",
    "df.iloc[3:5,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By lists of integer position locations, \n",
    "# similar to the numpy/python style\n",
    "df.iloc[[1,2,4],[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc is used to slice rows and columns explicitly\n",
    "df.iloc[1:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting a value explicitly\n",
    "df.iloc[1,1]\n",
    "# df.iat[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "\n",
    "We can use _boolean arrays_ to select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.A>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.A > 0]    # can you figure out intuitively what this does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['E'] = ['one', 'one','two','three','four','three']    # create a new column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['E'].isin(['two','four'])]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set data in a variety of ways\n",
    "print(df)\n",
    "df.at[dates[0],'A'] = 0               \n",
    "df.iat[0,1] = 0                       \n",
    "df.loc[:,'D'] = np.array([5] * len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['E']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# where operation with setting!\n",
    "df[df > 0] = -df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data\n",
    "\n",
    "Or data cleansing!\n",
    "\n",
    "Let's use some real data from Wikipedia!\n",
    "\n",
    "Pandas takes a \"batteries included approach\" and throws in a whole lot of convenience functions.  For instance it has import functions for a variety of formats.  One of the pleasant surprises is a command `read_html` that's meant to automate the process of extacting tabular data from HTML.  In particular, it works pretty well with tables on Wikipedia.  \n",
    "\n",
    "Let's do an example: We'll try to extract the list of the world's tallest buildings from\n",
    "http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_html('http://en.wikipedia.org/wiki/List_of_tallest_buildings_and_structures_in_the_world', header=0, parse_dates=False)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are several tables on the page.  By inspection we can figure out which one we want\n",
    "tallest = dfs[2]  \n",
    "tallest.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coordinates column needs to be fixed up. This needs a bit of string parsing (which you may be unfamiliar, but that's alright if you don't)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_lat_long(s):\n",
    "    try:\n",
    "        parts = s.split(\"/\")\n",
    "    except AttributeError:\n",
    "        return (None, None)\n",
    "    if len(parts)<3:\n",
    "        return None\n",
    "    m=re.search(r\"(\\d+[.]\\d+);[^\\d]*(\\d+[.]\\d+)[^\\d]\", parts[2])\n",
    "    if not m:\n",
    "        return (None, None)\n",
    "    return (m.group(1), m.group(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some new columns\n",
    "tallest['Clean_Coordinates'] = tallest['Coordinates'].apply(clean_lat_long)\n",
    "tallest['Latitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[0])\n",
    "tallest['Longitude'] = tallest['Clean_Coordinates'].apply(lambda x:x[1])\n",
    "\n",
    "# and voila...!\n",
    "tallest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional work: Check whether the data matches up with the one online and \n",
    "# correct the mistakes, if there are! Look out for the links in Year and Height\n",
    "# metres!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "When you read in a CSV file / SQL data base there is often \"NA\" (or \"null\", \"None\", etc.) values.  The CSV reader has a special field for specifying how this is denoted, and SQL has the built-in notion of NULL.  Pandas provides some tools for working with these.\n",
    "\n",
    "Note that these methods are by default not in place -- that is, they create a new series and do not change the original one.\n",
    "\n",
    "We're going to use a real data set, but only one column of it.  \n",
    "\n",
    "For more details: http://pandas.pydata.org/pandas-docs/stable/missing_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'first_name': ['Jason', np.nan, 'Tina', 'Jake', 'Amy'],\n",
    "        'last_name': ['Miller', np.nan, 'Ali', 'Milner', 'Cooze'],\n",
    "        'age': [42, np.nan, 36, 24, 73],\n",
    "        'sex': ['m', np.nan, 'f', 'm', 'f'],\n",
    "        'preTestScore': [4, np.nan, np.nan, 2, 3],\n",
    "        'postTestScore': [25, np.nan, np.nan, 62, 70]}\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'sex', 'preTestScore', 'postTestScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop missing observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing = df.dropna()\n",
    "df_no_missing\n",
    "#cleaned....but a bit strict.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows where all cells in that row is NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(how='all')\n",
    "df_cleaned\n",
    "# this may be better, some rows still might have information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column full of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop column if they only contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=1, how='all')\n",
    "# location column is now gone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows that contain less than five observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing data with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing in preTestScore with the mean value of preTestScore\n",
    "\n",
    "`inplace=True` means that the changes are saved to `df` right away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preTestScore\"].fillna(df[\"preTestScore\"].mean(), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing in postTestScore with each sex's mean value of postTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"postTestScore\"].fillna(df.groupby(\"sex\")[\"postTestScore\"].transform(\"mean\"), inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select some raws but ignore the missing data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the rows of df where age is not NaN and sex is not NaN\n",
    "df[df['age'].notnull() & df['sex'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Data\n",
    "\n",
    "The rest of this worksheet will use a single example, which contains data for customer counts per date at different store locations each week.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate test data\n",
    "def CreateDataSet(Number=1):\n",
    "    Output = []\n",
    "    for i in range(Number):\n",
    "        # Create a weekly (mondays) date range\n",
    "        rng = pd.date_range(start='1/1/2013', end='12/31/2016', freq='W-MON')\n",
    "        \n",
    "        # Create random data\n",
    "        data = np.random.randint(low=25,high=1000,size=len(rng))\n",
    "        \n",
    "        # Status pool\n",
    "        status = [1,2,3]\n",
    "        \n",
    "        # Make a random list of statuses\n",
    "        random_status = [status[np.random.randint(low=0,high=len(status))] for i in range(len(rng))]\n",
    "        \n",
    "        # State pool\n",
    "        location = ['Bangsar','Ampang','Petaling Jaya','Cheras']\n",
    "        \n",
    "        # Make a random list of states \n",
    "        random_location = [location[np.random.randint(low=0,high=len(location))] for i in range(len(rng))]\n",
    "\n",
    "        Output.extend(list(zip(random_location, random_status, data, rng)))\n",
    "        \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CreateDataSet(4)\n",
    "df = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to save this dataframe into an Excel file, to then bring it back to a dataframe. We simply do this to practice reading and writing to Excel files.  \n",
    "\n",
    "We do not write the index values of the dataframe to the Excel file, since they are not meant to be part of our initial test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Excel\n",
    "df.to_excel('data/Customers.xlsx', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of file\n",
    "Location = r'data/Customers.xlsx'\n",
    "\n",
    "# Parse a specific sheet - look what we did here!\n",
    "df = pd.read_excel(Location, 0, index_col='StatusDate')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the index of df\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pretend that status == only includes the people who \n",
    "# bought something. \n",
    "mask = df['Status'] == 1\n",
    "df = df[mask]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we may want to graph the data to check for any outliers or inconsistencies in the data. We will be using the ***plot()*** attribute of the dataframe.  \n",
    "\n",
    "As you can see from the graph below it is not very conclusive and is probably a sign that we need to perform some more data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CustomerCount'].plot(figsize=(15,5))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the data, we begin to realize that there are multiple values for the same location, StatusDate, and Status combination. It is possible that this means the data you are working with is dirty/bad/inaccurate, but we will assume otherwise. We can assume this data set is a subset of a bigger data set and if we simply add the values in the ***CustomerCount*** column per location, StatusDate, and Status we will get the ***Total Customer Count*** per day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortdf = df[df['location']=='Bangsar'].sort_index(axis=0)\n",
    "sortdf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Our task is now to create a new dataframe that compresses the data so we have daily customer counts per location and StatusDate. We can ignore the Status column since all the values in this column are of value *1*. To accomplish this we will use the dataframe's functions ***groupby*** and ***sum()***.  \n",
    "\n",
    "Note that we are using **reset_index** . If we did not, we would not have been able to group by both the location and the StatusDate since the groupby function expects only columns as inputs. The **reset_index** function will bring the index ***StatusDate*** back to a column in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by State and StatusDate\n",
    "Daily = df.reset_index().groupby(['location','StatusDate']).sum()\n",
    "Daily.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***location*** and ***StatusDate*** columns are automatically placed in the index of the ***Daily*** dataframe. You can think of the ***index*** as the primary key of a database table but without the constraint of having unique values. Columns in the index as you will see allow us to easily select, plot, and perform calculations on the data.  \n",
    "\n",
    "Below we delete the ***Status*** column since it is all equal to one and no longer necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Daily['Status']\n",
    "Daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another groupby!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the index of the Daily dataframe\n",
    "Daily.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the location index\n",
    "Daily.index.levels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try unstacking with 0 and 1.\n",
    "Daily.unstack(0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables\n",
    "\n",
    "\n",
    "Are easy! and are akin to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Create the data set again.\n",
    "dataset = CreateDataSet(4)\n",
    "df = pd.DataFrame(data=dataset, columns=['location','Status','CustomerCount','StatusDate'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values = 'CustomerCount', \n",
    "               index = ['StatusDate','Status'],\n",
    "               columns = ['location']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values = 'CustomerCount', \n",
    "               index = ['StatusDate','location'],\n",
    "               columns = ['Status']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now plot the data per location.  \n",
    "\n",
    "As you can see by breaking the graph up by the ***location*** column we have a much clearer picture on how the data looks like. Can you spot any outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for place in Daily.index.levels[0]:\n",
    "    print(place)\n",
    "    Daily.loc[place].plot()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that per month the customer count should remain relatively steady. Any data outside a specific range in that month will be removed from the data set. The final result should have smooth graphs with no spikes.  \n",
    "\n",
    "***LocationYearMonth*** - Here we group by location, Year of StatusDate, and Month of StatusDate.  \n",
    "***Daily['Outlier']*** - A boolean (True or False) value letting us know if the value in the CustomerCount column is ouside the acceptable range.  \n",
    "\n",
    "We will be using the attribute ***transform*** instead of ***apply***. The reason is that transform will keep the shape(# of rows and columns) of the dataframe the same and apply will not. By looking at the previous graphs, we can realize they are not resembling a gaussian distribution, this means we cannot use summary statistics like the mean and stDev. We use percentiles instead. Note that we run the risk of eliminating good data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Outliers\n",
    "LocationYearMonth = Daily.groupby([Daily.index.get_level_values(0), Daily.index.get_level_values(1).year, Daily.index.get_level_values(1).month])\n",
    "Daily['Lower'] = LocationYearMonth['CustomerCount'].transform( lambda x: x.quantile(q=.25) - (1.5*x.quantile(q=.75)-x.quantile(q=.25)) )\n",
    "Daily['Upper'] = LocationYearMonth['CustomerCount'].transform( lambda x: x.quantile(q=.75) + (1.5*x.quantile(q=.75)-x.quantile(q=.25)) )\n",
    "Daily['Outlier'] = (Daily['CustomerCount'] < Daily['Lower']) | (Daily['CustomerCount'] > Daily['Upper']) \n",
    "\n",
    "# Remove Outliers\n",
    "Daily = Daily[Daily['Outlier'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe named ***Daily*** will hold customer counts that have been aggregated per day. The original data (df) has multiple records per day.  We are left with a data set that is indexed by both the state and the StatusDate. The Outlier column should be equal to ***False*** signifying that the record is not an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We create a separate dataframe named ***ALL*** which groups the Daily dataframe by StatusDate. We are essentially getting rid of the ***Location*** column. The ***Max*** column represents the maximum customer count per month. The ***Max*** column is used to smooth out the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all markets\n",
    "\n",
    "# Get the max customer count by Date\n",
    "ALL = pd.DataFrame(Daily['CustomerCount'].groupby(Daily.index.get_level_values(1)).sum())\n",
    "ALL.columns = ['CustomerCount'] # rename column\n",
    "\n",
    "# Group by Year and Month\n",
    "YearMonth = ALL.groupby([lambda x: x.year, lambda x: x.month])\n",
    "\n",
    "# What is the max customer count per Year and Month\n",
    "ALL['Max'] = YearMonth['CustomerCount'].transform(lambda x: x.max())\n",
    "ALL.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the ***ALL*** dataframe above, in the month of Jan 2013, the maximum customer count was 1702. If we had used ***apply***, we would have got a dataframe with (Year and Month) as the index and just the *Max* column with the value of 1702. \n",
    "\n",
    "\n",
    "----------------------------------  \n",
    "There is also an interest to gauge if the current customer counts were reaching certain goals the company had established. The task here is to visually show if the current customer counts are meeting the goals listed below. We will call the goals ***BHAG*** (Big Hairy Annual Goal).  \n",
    "\n",
    "* 12/31/2015 - 1,000 customers  \n",
    "* 12/31/2016 - 2,000 customers  \n",
    "* 12/31/2017 - 3,000 customers  \n",
    "\n",
    "We will be using the **date_range** function to create our dates.  \n",
    "\n",
    "***Definition:*** date_range(start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None)  \n",
    "***Docstring:*** Return a fixed frequency datetime index, with day (calendar) as the default frequency  \n",
    "\n",
    "By choosing the frequency to be ***A*** or annual we will be able to get the three target dates from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presenting Data  \n",
    "\n",
    "Create individual Graphs for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# First Graph\n",
    "ALL['Max'].plot(figsize=(10, 5));plt.title('ALL Markets')\n",
    "\n",
    "# Last four Graphs\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=1.0) ## Create space between plots\n",
    "\n",
    "Daily.loc['Bangsar']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[0,0])\n",
    "Daily.loc['Petaling Jaya']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[0,1]) \n",
    "Daily.loc['Ampang']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[1,0]) \n",
    "Daily.loc['Cheras']['CustomerCount']['2016':].fillna(method='pad').plot(ax=axes[1,1]) \n",
    "\n",
    "\n",
    "\n",
    "# Add titles\n",
    "axes[0,0].set_title('Bangsar')\n",
    "axes[0,1].set_title('Petaling Jaya')\n",
    "axes[1,0].set_title('Ampang')\n",
    "axes[1,1].set_title('Cheras')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pandas provides a \"batteries-included\" basic data analysis:\n",
    "  - **Loading data:** `read_csv`, `read_table`, `read_sql`, and `read_html`\n",
    "  - **Selection, filtering, and aggregation** (i.e., SQL-type operations): There's a special syntax for `SELECT`ing.  There's the `merge` method for `JOIN`ing.  There's also an easy syntax for what in SQL is a mouthful: Creating a new column whose value is computed from other column -- with the bonus that now the computations can use the full power of Python (though it might be faster if it didn't).\n",
    "  - **\"Pivot table\" style aggregation**: If you're an Excel cognosceti, you may appreciate this.\n",
    "  - **NA handling**: Like R's data frames, there is good support for transforming NA values with default values / averaging tricks / etc.\n",
    "  - **Basic statistics:** e.g. `mean`, `median`, `max`, `min`, and the convenient `describe`.\n",
    "  - **Plugging into more advanced analytics:** Okay, this isn't batteries included.  But still, it plays reasonably with `sklearn`.\n",
    "  - **Visualization:** For instance `plot` and `hist`.\n",
    "  \n",
    "Plugging into more advanced analytics\n",
    "-------\n",
    "Almost any \"advanced analytics\" tool in the Python ecosystem is going to take as input `np.array` type arrays.  You can access the underlying array of a data frame column as\n",
    "\n",
    "        df['column'].values\n",
    "        \n",
    "Many of them take `nd.array` whose underlying data can be accessed by \n",
    "\n",
    "        df.values\n",
    "        \n",
    "directly.  *Most* of the time, they will take `df['column']` and `df` without needing to look at values.\n",
    "\n",
    "This is particularly important if you want to use Pandas with the sklearn library. See this [blog post](http://www.markhneedham.com/blog/2013/11/09/python-making-scikit-learn-and-pandas-play-nice/) for an example."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
